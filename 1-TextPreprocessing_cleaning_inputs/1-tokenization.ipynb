{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization - Text Preprocessing\n",
    "\n",
    "**Goal**: This notebook introduces the fundamental concept of tokenization in natural language processing (NLP). Tokenization is the process of splitting text into smaller units, such as words or sentences, which can be further processed by machine learning models.\n",
    "\n",
    "**Context**: Tokenization is a crucial first step in any NLP pipeline. It converts raw text into meaningful chunks or tokens that allow models to analyze and extract information. Understanding how to tokenize text is critical in fields like information retrieval, language modeling, and text classification. In this notebook, we explore different tokenization techniques and their impact on further text processing steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\bleew\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Hello.\n",
      "    Welcome to this example of Tokenization! I'm on my way to become an expert in NLP.\n",
      "    Please do look at the entire file.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = \"\"\"\n",
    "    Hello.\n",
    "    Welcome to this example of Tokenization! I'm on my way to become an expert in NLP.\n",
    "    Please do look at the entire file.\n",
    "\"\"\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Tokenization\n",
    "##  Sentence -> paragraphs\n",
    "from nltk.tokenize import sent_tokenize\n",
    "documents = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Hello.\n",
      "Welcome to this example of Tokenization!\n",
      "I'm on my way to become an expert in NLP.\n",
      "Please do look at the entire file.\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " '.',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'this',\n",
       " 'example',\n",
       " 'of',\n",
       " 'Tokenization',\n",
       " '!',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'on',\n",
       " 'my',\n",
       " 'way',\n",
       " 'to',\n",
       " 'become',\n",
       " 'an',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'look',\n",
       " 'at',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'file',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##  Tokenization\n",
    "##  Paragraph -> words\n",
    "##  Sentence -> words\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '.']\n",
      "['Welcome', 'to', 'this', 'example', 'of', 'Tokenization', '!']\n",
      "['I', \"'m\", 'on', 'my', 'way', 'to', 'become', 'an', 'expert', 'in', 'NLP', '.']\n",
      "['Please', 'do', 'look', 'at', 'the', 'entire', 'file', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " '.',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'this',\n",
       " 'example',\n",
       " 'of',\n",
       " 'Tokenization',\n",
       " '!',\n",
       " 'I',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'on',\n",
       " 'my',\n",
       " 'way',\n",
       " 'to',\n",
       " 'become',\n",
       " 'an',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'look',\n",
       " 'at',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'file',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Splits puntuactions\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello.',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'this',\n",
       " 'example',\n",
       " 'of',\n",
       " 'Tokenization',\n",
       " '!',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'on',\n",
       " 'my',\n",
       " 'way',\n",
       " 'to',\n",
       " 'become',\n",
       " 'an',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'look',\n",
       " 'at',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'file',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Does not splits some punctuations from words \n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
